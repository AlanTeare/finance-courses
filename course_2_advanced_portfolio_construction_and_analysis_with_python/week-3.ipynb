{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "import edhec_risk_kit as erk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using seaborn style (type plt.style.available to see available styles)\n",
    "plt.style.use(\"seaborn-dark\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimates for expected returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In principle, the **Modern Portfolio Theory** (the mean-variance approach of **Markowitz**) offers a solution to this problem once the expected returns and covariances of the assets are known. While Modern Portfolio Theory is an important theoretical advance, **its application has universally encountered a problem**: although the covariances of a few assets can be adequately estimated, **it is difficult to come up with reasonable estimates of expected returns**.\n",
    "\n",
    "In particular, recall that Markowitz portfolio allocation approach takes in input the set of expected returns and expected volatilities of the assets we are interested in, and gives in output the set of optimal weights in these assets. \n",
    "\n",
    "The problem is that **very small changes in the expected returns normally result in large changes in output weights**. Of course, this represent a problem (also because we cannot pretend to invest today in a portoflio by relying on exact past returns and volatilities as they are not likely to be the same in future).\n",
    "\n",
    "In general, **sample based information** such as the returns computed using sampled past data **are close to useless** \n",
    "\n",
    "Let $N$ be the number of assets in our portfolio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shrinked expected returns \n",
    "\n",
    "A very basic estimate for expected returns is a simple modificiation of sample means. Let \n",
    "$$\n",
    "\\mu_i := \\frac{1}{T} \\sum_{t=1}^{T} r_{t}^{i} \n",
    "\\quad\\text{and}\\quad\n",
    "\\bar{\\mu} := \\frac{1}{N} \\sum_{i=1}^N \\mu_i, \n",
    "$$\n",
    "be the **sample mean return** of each asset \n",
    "and the **grand sample mean** (i.e., the mean of the computed mean returns), respectively. \n",
    "The sample mean might be improved by **shrinking** the individual means to the grand sample mean:\n",
    "$$\n",
    "\\hat{\\mu}_i := \\delta \\bar{\\mu} + (1-\\delta)\\mu_i,\n",
    "$$\n",
    "given a shrinking factor $\\delta\\in [0,1]$ (when $\\delta=1$ we are saying that all asset's returns are equal to the grand sample mean)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample means:   [11.1 12.3  8.4 15.3  5.6  9.7]%\n",
      "shrinked means: [10.75 11.35  9.4  12.85  8.   10.05]%\n"
     ]
    }
   ],
   "source": [
    "# these are the \\mu_i\n",
    "asset_means_rets = np.array([11.1, 12.3, 8.4, 15.3, 5.6, 9.7])/100\n",
    "\n",
    "# this is \\bar{\\mu} \n",
    "grand_sample_mean = asset_means_rets.mean()\n",
    "\n",
    "# shrinked mean\n",
    "delta = 0.5\n",
    "shrinked_mean_ret = delta * grand_sample_mean + (1-delta)* asset_means_rets\n",
    "\n",
    "print(\"sample means:   {}%\" .format( np.round(asset_means_rets*100,2)) )\n",
    "print(\"shrinked means: {}%\" .format( np.round(shrinked_mean_ret*100,2) ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relying on agnostic priors\n",
    "\n",
    "Recall that the **expected return and the volatility of a portfolio** of $N$ assets are given by:\n",
    "$$\n",
    "\\mu_p := \\mathbf{w}^T\\mathbf{\\mu} = \\sum_{i=1}^N w_i \\mu_i\n",
    "\\quad\\text{and}\\quad\n",
    "\\sigma_p = (\\mathbf{w}^T \\Sigma \\mathbf{w})^{1/2}, \n",
    "$$\n",
    "where $\\mathbf{w}:=(w_1,\\dots,w_N)^T$ is the vector of weigths, \n",
    "$\\mathbf{\\mu}:=(\\mu_1,\\dots,\\mu_N)$, where $\\mu_i=\\mathbb{E}[r_i]$ for each $i=1,\\dots,N$ is the expected return of asset $i$, and $\\Sigma$ is the expected covariance matrix of the assets.\n",
    "\n",
    "The vector of expected returns $\\mathbf{\\mu}:=(\\mu_1,\\dots,\\mu_N)$ is what we are mainly concerned about. \n",
    "\n",
    "We have to introduce some, ideally **economically motivated, priors** since using sample based expected returns is not going to work fine.\n",
    "\n",
    "\n",
    "Let us suppose that we are interested in the maximum sharpe ratio portfolio, i.e., the portoflio or which the sharpe ratio:\n",
    "$$\n",
    "\\lambda_p = \\frac{\\mu_p - r_f}{\\sigma_p} = \\frac{ \\sum_{i=1}^N w_i (\\mu_i - r_f)}{ (\\mathbf{w}^T \\Sigma \\mathbf{w})^{1/2} },\n",
    "$$\n",
    "is maximum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First agnostic prior: all returns are equal\n",
    "A very first agnostic prior is assuming that **all expected returns are going to be equal**, for example, equal to the grand sample mean. Of course we know that such assumption would be wrong. However, \n",
    "in this case we have:\n",
    "$$\n",
    "\\mu_i - r_f = \\hat{\\mu} - r_f \\qquad \\forall\\; i,\n",
    "$$\n",
    "hence maximizing the sharpe ratio means, indeed, minimizing the volatility:\n",
    "$$\n",
    "\\lambda_p = \\frac{\\mu_p - r_f}{\\sigma_p} \n",
    "= \\frac{ \\sum_{i=1}^N w_i (\\hat{\\mu} - r_f)}{ (\\mathbf{w}^T \\Sigma \\mathbf{w})^{1/2} }\n",
    "= \\frac{ \\hat{\\mu} - r_f }{ (\\mathbf{w}^T \\Sigma \\mathbf{w})^{1/2} }.\n",
    "$$\n",
    "\n",
    "#### Second agnostic prior: sharpe ratios are all equal\n",
    "A second agnostic prior would be assuming that sharpe ratios are all equal, in particular, this means assuming that the excess \n",
    "return $\\mu_i-r_f$ of each asset is proportional to the corresponding volatility $\\sigma_i$:\n",
    "$$\n",
    "\\mu_i - r_f = \\lambda \\sigma_i \n",
    "\\qquad  \\forall\\; i, \n",
    "$$\n",
    "where the constant of proportionality $\\lambda$ is sharpe ratio supposed to be the same for each asset. In this case, the assumption we are making is simply that **more volatile assets are going to have larger expected returns**. The sharpe ratio of the portfolio becomes:\n",
    "$$\n",
    "\\lambda_p = \\frac{\\mu_p - r_f}{\\sigma_p} \n",
    "= \\frac{ \\sum_{i=1}^N w_i \\lambda \\sigma_i}{ (\\mathbf{w}^T \\Sigma \\mathbf{w})^{1/2} }\n",
    "= \\lambda \\frac{ \\sum_{i=1}^N w_i \\sigma_i }{ (\\mathbf{w}^T \\Sigma \\mathbf{w})^{1/2} }. \n",
    "$$\n",
    "The ratio between the weigthed volatilities and the volatility of the portfolio is known as the **diversification ratio**.\n",
    "\n",
    "**EXAMPLE:** \n",
    "What is the Sharpe ratio of a portfolio of an equally-weighted portfolio of two stocks with volatility equal to $20\\%$ and $30\\%$, respectively, and a pairwise correlation $0.6$, assuming that they \n",
    "both have a Sharpe ratio of $\\lambda = 70\\%$?\n",
    "\n",
    "First of all, we have $\\sigma_1=0.2$, $\\sigma_2=0.3$ and $\\rho_{12}=0.6$. The covariance of the stocks is $\\text{Cov}(r_1,r_2) = \\rho_{12}\\sigma_1\\sigma_2 = 0.6\\cdot 0.2\\cdot 0.3 = 0.036$. Then:\n",
    "$$\n",
    "\\lambda_p = \\frac{\\mu_p - r_f}{\\sigma_p} \n",
    "= \\lambda \\frac{ \\sum_{i=1}^2 w_i \\sigma_i }{ (\\mathbf{w}^T \\Sigma \\mathbf{w})^{1/2} } \n",
    "= 0.7 \\frac{ 0.5\\cdot 0.2 + 0.5\\cdot 0.3 }{ \n",
    "\\sqrt{\n",
    "(0.5, 0.5) \n",
    "\\begin{pmatrix}\n",
    "0.04 & 0.036 \\\\\n",
    "0.036 & 0.09 \n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "0.5 \\\\\n",
    "0.5\n",
    "\\end{pmatrix}\n",
    "} \n",
    "}\n",
    "=\n",
    "0.7 \\frac{ 0.25 }{ \\sqrt{0.0505} } = 77.87\\%.\n",
    "$$\n",
    "\n",
    "Do we believe that all stocks and all components have the same Sharpe ratio? \n",
    "It is a very strong assumption and we have reasons to believe that not all of risk is rewarded. \n",
    "In fact, asset pricing theory suggests that **only the systematic component is rewarded** and \n",
    "the specific risk is not rewarded because it can be diversified away. In other words, asset pricing theory suggests that **we should not be rewarded for a piece of risks that we could have diversified away in the first place** and we should assume that \n",
    "**there is a relationship between excess expected return and not total risk, but the systematic part of volatility**. \n",
    "\n",
    "We want to **decompose volatility in terms of specific risk and systematic risk**, \n",
    "and come up with a better **estimate for expected returns by relating it to systematic risk**. We could do this using a factor model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Blackâ€“Litterman overcame this problem by not requiring the user to input estimates of expected return; instead it assumes that the initial expected returns are whatever is required so that the equilibrium asset allocation is equal to what we observe in the markets. The user is only required to state how his assumptions about expected returns differ from the markets and to state his degree of confidence in the alternative assumptions. From this, the Blackâ€“Litterman method computes the desired (mean-variance efficient) asset allocation.\n",
    "\n",
    "In general, when there are portfolio constraints - for example, when short sales are not allowed - the easiest way to find the optimal portfolio is to use the Blackâ€“Litterman model to generate the expected returns for the assets, and then use a mean-variance optimizer to solve the constrained optimization problem\n",
    "\n",
    "\n",
    "\n",
    "Black-Litterman model is based on an assumption that asset returns have greatest impact to portfolio weightings in mean-variance optimization. It is therefore attempting to reverse-engineer these returns from index constituents rather than relying on historical data. This results into several advantages including but not limited to:\n",
    "Intuitive portfolios with weightings not too much different from benchmark indices.\n",
    "Ability to incorporate custom views\n",
    "Lower tracking error and reliance on historical data\n",
    "Greater stability of efficient frontier and better diversification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
